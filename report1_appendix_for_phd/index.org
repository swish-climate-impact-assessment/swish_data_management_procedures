#+TITLE:SWISH DMP 
#+AUTHOR: Ivan Hanigan
#+email: ivan.hanigan@anu.edu.au
#+LaTeX_CLASS: article
#+LaTeX_CLASS_OPTIONS: [a4paper]
#+LATEX: \tableofcontents
-----
* Run the Rmd
#+begin_src R :session *R* :tangle no :exports none :eval yes :padline yes
  
  # func
  setwd("~/projects/swish-dmp/")
  library(rmarkdown)
  library(knitr)
  library(knitcitations)
  library(bibtex)
  cleanbib()
  # rm("bib")
  #options("cite_format"="pandoc")
  cite_options(citation_format = "pandoc", check.entries=FALSE)
  
  #rmarkdown::render("swish-dmp-report.Rmd", "html_document")  
  rmarkdown::render("swish-dmp-report.Rmd", "pdf_document")
  # browseURL("swish-dmp-report.pdf")
  # browseURL("swish-dmp-report.html")
#+end_src  

#+RESULTS:
: /home/ivan_hanigan/projects/hanigan-thesis/thesis/swish-dmp-report.pdf

* head
#+begin_src R :session *R* :tangle swish-dmp-report.Rmd :exports none :eval no
---
title: "Data management for implementing an eco-social epidemiologic research programme"
author: Ivan C. Hanigan
header-includes:
  - \usepackage{graphicx}
  - \usepackage{setspace}
output:
  html_document:
    toc: true
    theme: united
    number_sections: yes    
    toc_depth: 2
  pdf_document:
    toc: true
    toc_depth: 2
    highlight: zenburn
    keep_tex: true
    number_sections: yes        
documentclass: article
classoption: a4paper
csl: meemodified.csl
bibliography: references.bib
---

\doublespacing

```{r echo = F, eval=F, results="hide"}

# func
setwd("~/projects/swish-dmp/")
library(rmarkdown)
library(knitr)
library(knitcitations)
library(bibtex)
cleanbib()
#rm("bib")
#options("cite_format"="pandoc")
cite_options(citation_format = "pandoc", check.entries=FALSE)

#rmarkdown::render("Hanigan-Thesis-Bridging.Rmd", "html_document")
#rmarkdown::render("Hanigan-Thesis-Papers.Rmd", "html_document")

rmarkdown::render("Hanigan-Thesis-Bridging.Rmd", "pdf_document")
#rmarkdown::render("Hanigan-Thesis-Papers.Rmd", "pdf_document")
#rmarkdown::render("suiclim.Rmd", "pdf_document")
# rmarkdown::render("chap-synthesis.Rmd", "pdf_document")
#rmarkdown::render("chap-synthesis.Rmd", "html_document")
#browseURL("Hanigan-Thesis-Bridging.pdf")
# browseURL("suiclim.pdf")

# to make on editable
# browseURL("Hanigan-Thesis-Bridging.html")
# Then use word to copy/paste into the doc

# also tried
#knit2html("Hanigan-Thesis-Bridging.Rmd", options = c("toc", markdown::markdownHTMLOptions(TRUE)), sylesheet='custom.css')
#system("pandoc -i Hanigan-Thesis-Bridging.md -o Hanigan-Thesis-Bridging.html --bibliography references.bib")
#rmarkdown::render("Hanigan-Thesis-Bridging.Rmd", "word_document")
# browseURL("Hanigan-Thesis-Bridging.html")
#system("pandoc -V papersize:'a4paper' -i Hanigan-Thesis-Bridging.html -o Hanigan-Thesis-Bridging.docx ")

# but all have issues with the tables

```
#+end_src

* Implementing Reproducible research pipelines in epidemiology
** 2.1 intro to R epi 
*** snip-code
## Introduction to reproducible epidemiological data science
  The details of the scientific questions that motivated the work on
  reproducibility are not included, however throughout the discussion
  there is the imperative that the techniques suite the
  context of environmental epidemiology.  This requirement guided
  the selection of the material included.  Other scientific disciplines
  may find different components of pipelines are required for specific
  data collection and management techniques. 
*** Rmd
#+begin_src R :session *R* :tangle swish-dmp-report.Rmd :exports none :eval no
  
  # Implementing reproducible research pipelines in epidemiology
  
  
  ## Configuration versus convention: the case for standardised approaches  

  
  In the first chapter the concepts of reproducibility and replication
  were explained.  These concepts are central to the emerging field of
  data science.  Here, the focus is on reproducibility in particular.
  Overall, there is a guiding principle that for the effective
  implementation of pipelines, scientists should employ standardised and
  evidence-based methods based on conventions developed from many data
  analysts approaching the problems in a similar way, rather than each
  analyst configuring a pipeline to suit a particular individual or
  domain-specific preferences.
  
  `r citet(bib[["Noble2009"]])` points out that
  'the principles behind organizing and documenting computational experiments are often learned on the fly, and this learning is strongly influenced by personal predilections'.
  `r citet(bib[["Leek2015b"]])` describe this as data analysis being
  'taught through an apprenticeship model, and different disciplines develop their own analysis subcultures'. By
  codifying what an appropriate pipeline would contain, data analysis
  will be more robust.  According to `r citet(bib[["Peng"]])`, there
  should not be a 'lonely data analyst' coming up with their own
  method. If a researcher conducted an analysis using a reproducible
  pipeline
  'you could at least have a sense that something reasonable was done'
  and be confident that you could easily check what had been done if you needed to.
#+end_src
*** COMMENT snip-code
#+name:snip
#+begin_src R :session *R* :tangle no :exports none :eval no
  #### name:snip ####

    
  However, in many cases it is hard or impossible to
  exactly replicate an epidemiological study: time moves on, the
  demographics change, the drought, smog or smoke conditions can\'t be
  replicated. Even so, in principle, one might
  reasonably conduct a similar study and obtain similar health results.
  In practice it has been found that, because of the time and expense
  constraints of many epidemiological studies, it is unrealistic to
  expect replication of findings. A pragmatic alternative has been
  proposed because it is recognised that a more attainable minimum
  standard is 'reproducibility', which calls for data sets and software
  to be made available for verifying published findings and conducting
  alternative analyses `r citep(bib[["Peng2006"]])`.  

  This model has been referred to in general as the reproducible
  research pipeline by `r citet(bib[["Solymos2008"]])` who created the graphical view shown in Figure \ref{fig:reproduciblepipeline}. This model is also sometimes called the 'data science pipeline'
  `r citep(bib[["Peng"]])` and is related to the practice of
  'evidence based data analysis' (where tools and techniques are applied
  based on knowledge of their effectiveness, not just on assumptions by
  the analyst).  


#+end_src
*** Core components of pipeline
#+begin_src R :session *R* :tangle swish-dmp-report.Rmd :exports none :eval no
  ### The core components of a pipeline
  As mentioned in chapter 1, `r citet(bib[["Peng2006"]])` distilled a core set of components for reproducibility from earlier work including that of `r citet(bib[["Schwab2000"]])`.  These are:
  
  - Hypothesis and design
  - Data (measurement, pre-processing, analytic)
  - Analysis Methods
  - Documentation (of all steps)
  - Distribution (of the paper, data and code).
   
  In essence this requires attention to be turned to the issue of how
  the data and analytical steps amassed – toward a reality where this is
  archived and there is a good understanding all round as to how the
  study were set up and conducted.  Different assumptions or different
  treatment of the data could conceivably lead to different inferences
  and conclusions being drawn, such as in the example where 29 research
  teams were given the same dataset but reached a wide variety of
  conclusions using different methods on the same data set to answer the
  same question (about football players’ skin colour and red cards)
  `r citep(bib[["Silberzahn2015"]])`.
  
  This is partly because of an underlying complexity in the information
  drawn from complex systems involving multi-causality, and partly
  because of different assumptions and different backgrounds and
  viewpoints. A finding that a variable does or does not cause a
  disease, might be drawn honestly from the same set of data.
  
  The techniques of pipelines described here are targeting the integrity
  of the process of data selection, the robustness and suitableness of
  the methods used, a commonsense and well-argued selection of health
  outcomes and environmental or social exposures, and the clarity and
  transparency of the assumptions made.
  
  
  ### Hypothesis and design
  
  The first stage of the pipeline is hypothesis generation and study
  design.  In this stage documentation should explain the literature
  base supporting the study, the decisions made in selection of explanatory
  factors for inclusion, decisions made such as the experimental unit,
  observational unit, measurement method, as well as spatial or temporal
  extent.  This information will also be needed for ethical review and approval.
  
  ### Data
  
  The data that were measured should be well managed, however the
  requirements for accessing the original raw data are less important
  than for the analytical dataset.  Descriptions of how the measured
  data were transformed into the analytic data should be available.
  Public data repositories or institutional services such as university
  libraries should be used to ensure longevity of the data storage.  
  
  ### Methods
  
  The software code underlying the principal
  results needs to be made available. In
  addition, the computer environment necessary to execute that code should be
  described adequately to 'deploy' a new computer set-up
  that can reproduce the computations needed.
  
  ### Documentation
  
  Adequate documentation of the code and
  data should be available to enable others to repeat the
  analyses and to conduct other similar ones.  This can take the form of metadata, reports, journal papers or even books `r citep(bib[["Peng2008a"]])`. Indeed textbooks on statistical methods can benefit greatly from being accompanied by data and analytical code to enhance their pedagogic functions `r citep(c(bib[["Barnett2015"]], bib[["Barnett2010"]]))`.
  
  An important underpinning to reproducible research is the
  reproducible report.  This is the ultimate form of documentation
  because the information that represents the outputs of the research is
  written alongside the code that performs the computations that are
  being described.  There has been many recent advances made in terms of
  tools for reproducible reports such as R markdown and knitr `r citep(bib[["Xie2014a"]])`.
  
  Metadata should be created and maintained as a priority task at all
  stages of the data analysis process.  An international standard should
  be preferred over selectively choosing what information one collects
  and what fieldnames one uses to describe each item of documentation.
  Ecological Metadata Language (EML) and the Data Documentation
  Inititative (DDI) are two such standards that offer useful ontological
  constructs for describing epidemiological data.
  
  ### Distribution
  
  Distribution or dissemination of the material needs to use a standard
  method if they are to be used by others.  It is not enough just to
  provide access to the software and data, but also adequate
  documentation is required to explain and potentially assist downstream
  users to piece these together.
  
#+end_src
** 2.2 fundamentals
*** Rmd
#+begin_src R :session *R* :tangle swish-dmp-report.Rmd :exports none :eval no
  
  
  
  ## Procedures when conducting a reproducible research analysis
  
  Having defined above the principle components for a pipeline there are procedural questions about how to go about compiling those. The key steps include:
  
  - Data Management Plans and Data Inventories
  - Tracking method steps
  - Developing code
  - Maintaining data storage
  - Writing reports
  - Distributing the materials.
  
  ### Data management plan and data inventory
  
  In environmental epidemiology there is a need for a data management
  plan and a data inventory that enables the scientist (or
  multidisciplinary teams of scientists) to manage a large and
  heterogeneous collection of disparate data sources efficiently.
  Keeping track of all the elements of a linked health, social and
  environmental database is very challenging, despite major improvements
  in data management software, web-portals and virtual laboratories
  `r citep(bib[["Fleming2014"]])`.
  
  Effective data management policies and procedures are essential in
  managing data-related risk. Such risks include data loss or
  corruption, technological obsolescence, breaches of privacy or
  copyright, and errors or misuse.  Misuse may be due either to
  unintended user misunderstandings about data attributes (no dataset is
  perfect and self-explanatory, see `r citet(bib[["Michener1997"]])`) or
  intentional mis-use for malicious or selfish reasons (for example the
  misuse of data by Bjorn Lomborg to support the argument that
  environmental health conditions are actually improving.  See
  `r citet(bib[["Bodnar2004"]])` for a discussion on Lomborg\'s misuse
  of data.  There have also been examples of mistakes in climate change
  science `r citep(bib[["Cai2010"]])`.  The careful storage and curation
  of datasets is critical—data from many studies are lost
  `r citep(c(bib[["Pullin2010"]], bib[["Vines2014a"]]))`.
  
  Data management plans are needed for developing procedures and
  processes to keep data safe.  There is an issue when ensuring that all
  relevant data are collected in deciding what is relevant.  Keeping an
  up-to-date data inventory and careful organisation of all folders and
  files helps mitigate these problems.
  
  Whether data management is the responsibility of the individuals
  collecting or collating it, or of the lead scientist, clarity on how
  and where data are stored and who manages it is vital, as is a
  'succession plan' that sets out the vision of the data collections
  preservation and re-use into the future.
  
  ### Metadata standards
  
  For data to be reused in the future, metadata and documentation need to be carefully
  prepared to allow future users (including the original collector) to find and understand the
  data (Michener et al. 1997). Metadata should be associated with the data
  and adhere to a standard schema. A number of metadata schemas are available to choose
  from. For example, the Dublin Core is a general international standard for metadata, while
  domain-specific schemas include the Ecological Metadata Language (EML) for ecology, and the
  Data Documentation Initiative (DDI) for social science. Good metadata requires sufficient detail
  to describe the collection process and to record decisions that were made during the
  design phase about the use of different sampling methods. Time and effort may be saved
  by considering metadata requirements at the commencement of a study, rather than trying
  to recall all the details later. If metadata adheres to a standard schema, it can be used in
  catalogues to enable fast searching and retrieval, or in machine-to-machine data queries
  that assist data access and use.
  
  In this thesis two metadata standards were explored: the DDI and the
  EML. The EML standard was chosen for processing most of the data
  publications of this thesis as it is also used internationally by
  large organisations who are actively developing new approaches to data
  distribution.  These include the Data Observation Network for Earth
  (DataONE) nodes, the United States Long Term Ecological Research (US
  LTER) network and the International Long Term Ecological Research
  (ILTER) network. Additionally, the standard is used in Australia at
  organisations such as the Long Term Ecological Research Network where
  Dataset 2 of this thesis was published.
  
  In EML the elements of any dataset can be seen as nested at three levels:
   
  1. The Project level: this is an overarching grouping of data.  It might be indicative of the principal investigator or organisation who provided the data, or a programme of research studies.
  2. The Dataset level: this is a distinct grouping of data that might be organised around a particular time period or geographical region.
  3. The Entity level: This grouping of data includes data files (such as tables in CSV or Excel, shapefiles and raster images) or documents (such as metadata descriptions or related publications). 
  
  This conceptual framework can be very useful for the organisation of the work constituting a single pipeline, as well as when working with multiple pipelines within several projects.
  
  
#+end_src

*** box1
#+name:box1
#+begin_src R :session *R* :tangle swish-dmp-report.Rmd :exports none :eval no
  
  
  
  
  ### Case study 1: visualisation of methods steps using bespoke software
  
  The method step is the key atomic unit of a scientific pipeline.  It consists of inputs, outputs and a rationale for why the step is taken.
  
  A simple way to keep track of the steps, inputs and outputs is shown in Table \ref{tab:TableBasic}.
  
  ```{r results='asis', echo=FALSE}
  library(stringr)
  steps <- read.csv(textConnection('
  CLUSTER ,  STEP    , INPUTS                  , OUTPUTS                   
  A  ,  Step1      , "Input 1, Input 2"       , "Output 1"                 
  A  ,  Step2      , Input 3                  , Output 2                   
  B  ,  Step3      , "Output 1, Output 2"      , Output 3                  
  '), stringsAsFactors = F, strip.white = T)
  
  #kable(
  steps <- steps[,c("STEP", "INPUTS", "OUTPUTS")]
  library(xtable)
  #print(xtable(steps, caption = 'Simple', label = 'tab:TableBasic'), type = "html")
  
  
  tabcode <- xtable(steps, caption = 'A simple table to track method steps, data inputs and outputs', label = 'tab:TableBasic')
  align(tabcode) <-  c( 'l', 'p{.6in}','p{2in}','p{2in}' )
  #sink(paste(fname, '.tex',sep = ""))
  #cat(txt)
  print(tabcode,  include.rownames = F, table.placement = '!h',
   caption.placement = 'top', comment = F) #, type = "html")
  #rws <- seq(1, (nrow(steps)), by = 2)
  #col <- rep("\\rowcolor[gray]{0.95}", length(rws))
  #print(tabcode, booktabs = TRUE, include.rownames = F, table.placement = '!ht',
  # caption.placement = 'top',
  # add.to.row = list(pos = as.list(rws), command = col),
  # comment=FALSE)
  
  ```  
  
  The steps and data listed in Table \ref{tab:TableBasic} can be
  visualised.  To achieve this an R function was written as part of this
  PhD project and is demonstrated in case study 1: below.  The code is distributed in the author\'s own R package
  available on Github
  (\url{https://github.com/ivanhanigan/disentangle}).  This is the
  `newnode` function.  The function returns a string of text written in
  the `dot` language which can be rendered in R using the `DiagrammeR`
  package, or the standalone `graphviz` package.  This creates the graph
  view shown in Figure \ref{fig:FigBasic}.  Note that a new field was
  added for Descriptions as these are highly recommended.
  
  ```{r echo=T, eval=F}
  library(disentangle); library(stringr); library(readxl)
  steps <- read_excel("steps_basic_workflow.xlsx")
  nodes <- newnode(indat = steps, names_col = "STEP",
                   in_col = "INPUTS",out_col = "OUTPUTS")
  DiagrammeR::grViz(nodes)
  ```
  
  
  
  \begin{figure}[!ht]
  \centering
  \includegraphics[width=.5\textwidth]{fig-basic.pdf}
  \caption{A graphical view of the steps that comprise a simple data analysis pipeline}
  \label{fig:FigBasic}
  \end{figure}
  \clearpage
#+end_src
** data storage and access
*** Rmd
#+begin_src R :session *R* :tangle swish-dmp-report.Rmd :exports none :eval no
  
  ### Data storage and access 
  
  Some datasets such as sensitive personal information about suicide or
  climate change scenarios with restrictions due to privacy and
  confidentiality rules, or because of protected intellectual property,
  need to be accessed in a restricted way.  This complicates the
  implementation of the method of pipelines which dictates that all the
  steps, models and assumptions need to be made transparent and
  available for scientific debate even though the datasets may require
  authorisation to access.  Restrictions around access to data have
  increased recently in Australia.  As an example the custodians of the
  national mortality database made it virtually impossible to access
  these data for several years after the discovery of an incident in
  which Australian population health researcher Dr Stephen Begg had
  hacked into the database in an illegal act
  `r citep(bib[["OKeefe2007"]])`.  The subsequent investigation by the
  data custodians led to a wide ranging modification to the procedures
  for approval and provision of these data that make the access much
  more restricted.  Appropriate access to data is therefore required to
  address this issue. In the work reported in the conference
  presentation in this thesis, a range of available workflow tools for
  data management and analysis were investigated and developed.
  
  ### Reports 
  
  Reproducible research reports are written using a scripting language
  for statistical computing and graphics. The report is made up of
  ordinary text written in a suitable format that enables the
  computational process to recognise it as text. An example is the
  Rmarkdown format which is very similar to text used when authoring
  word processor documents (http://rmarkdown.rstudio.com). There are
  also chunks of pure statistical programming code (such as R codes)
  that perform data manipulations and analyses when the document is
  'evaluated'.  When the processing stage is run a report document is
  generated that includes both content as well as the output of any
  embedded computer code 'chunks' within the document.  An example of
  this is provided in the Supporting Information document of Paper 1 of
  this thesis.
   
  
#+end_src
*** DONE rational for naming and shaming Dr Begg
Hi Steve,

Good point, I actually want to attribute this data misuse to the researchers not the school.
It seems to me I can only really pin this directly to Dr Begg, per the article http://www.theaustralian.com.au/higher-education/hackers-pick-up-uq-cash-prize/story-e6frgcjx-1111113191659

"Stephen Begg was the person most responsible for the breaches...
...Associate Professor Vos, though not a user of confidential unit record files, cannot apply for access until June 1...
...It was not made clear in the letter whether Professor Vos was involved in the hacking. The university would not comment..."

I think I need to name him because this puts Begg up there with the horror stories I cite in another section (using authors names) suggesting misconduct during publication ('insert statistical method here' and 'where are the data?  Emma just make one up').  This is probably as significant a data misuse as the much touted LaCour or Potti falsifications.

I also think the magnitude of the impact on pop health researchers over the last several years is worth noting, but do you know if I overstate the importance of the hacking when it came to the coroners locking up the data?  As I recall the conversations we had with staff at the Dept Justice a couple years back, they verbally implicated that incident as the watershed moment when they decided no-one would access the mortality data for several years... but I am not sure it is actually on the formal record as such and I might overstep bounds of libel by making the direct link from Begg's hacking to Coroner's data lock-down?

I say: "Restrictions around access to data have increased recently in
Australia.  As an example the custodians of the national mortality
database made it virtually impossible to access these data for several
years after the discovery of an incident in which Australian
population health researcher Dr Stephen Begg had hacked into the
database in an illegal act (O'Keefe 2007).  The subsequent
investigation by the data custodians led to a wide ranging
modification to the procedures for approval and provision of these
data that make the access much more restricted."



On Thu, Dec 3, 2015 at 10:00 AM, Steven McEachern <steven.mceachern@anu.edu.au> wrote:

    HI Ivan,

    I’ve had a closer look, and this looks ok to me.

    The only thing I’d change is the specific naming of the UQ School of Population Health. It’s important to point out the hacking and it’s implications, but I’d probably just leave the reference and citation rather than naming and shaming in the paper itself (unless you want to name the hackers as well?). 

    Cheers,
    Steve


** snip-code
*** snip
Who decides?  In many cases people preserve data 'in case' and, if
  so, some of it turn out to be considered junk that later causes a
  problem. 


### Code

In this section the basic scripted workflow is described using R
and STATA as example.  I might mention workflow software such as
Kepler, VisTrails, Taverna, Ruffus.  I might also mention electronic
notebooks and IDE such as Rstudio, Notepad++, Emacs, Eclipse and
IPython (Jupyter). But these things are outside the scope of this
paper.

- TODO
### Backups and security

Where possible, copies need to be stored in multiple places to avoid
the consequences of catastrophes such as fire or flooding
`r citep(c(bib[["Hook2010"]], bib[["White2013b"]]))`.

- TODO maybe add version control here?

## The purpose of RRReports
  The purpose of this is to transparently document the examination of,
  and any modifications to the data that have been provided into the
  analysis pipeline. The motivation for examining data and 
  modifying its structure is borne out of the need to publish data that
  is understandable to the end user and as free from inconsistencies as
  possible.

*** pres
**** COMMENT pres1
#+name:pres
#+begin_src R :session *R* :tangle no :exports none :eval no
  #### name:pres ####
    library(stringr)
    steps <- read.csv(textConnection('
    CLUSTER ,  STEP    , INPUTS                  , OUTPUTS                   
    A  ,  Step1      , "Input 1, Input 2"       , "Output 1"                 
    A  ,  Step2      , Input 3                  , Output 2                   
    B  ,  Step3      , "Output 1, Output 2"      , Output 3                  
    '), stringsAsFactors = F, strip.white = T)
  write.csv(steps, "steps_basic_workflow.csv", row.names = F)
#+end_src
*** COMMENT steps_basic_workflow
#+name:steps_basic_workflow
#+begin_src R :session *R* :tangle steps_basic_workflow.R :exports none :eval no
  library(disentangle); library(stringr); library(readxl)
  steps <- read_excel("steps_basic_workflow.xlsx")
  nodes <- newnode(indat = steps, names_col = "STEP",
                   in_col = "INPUTS",out_col = "OUTPUTS")
  DiagrammeR::grViz(nodes)
  
  
#+end_src

** 2.3 DEPRECATED, ALREADY DISCUSSED distro
#+begin_src R :session *R* :tangle no :exports none :eval no
  ## Distributing data, code and documentation
  
  - This section is still TODO
  - Sharing computer code via Github
  - Publishing well documented data with metadata in a standard format (EML, DDI, ANZLIC)
#+end_src
** 2.4 NOT NEEDED planning
#+begin_src R :session *R* :tangle no :exports none :eval no


  
  ## Planning a pipeline
  
  - This section is still TODO
  
  The ordering of the process is roughly (after `r citet(bib[["Peng"]])`):
  
  1. Decide on a research question 
  1. Select a modelling framework 
  1. Conceptualise the ideal analysis data
  1. Acquire and pre process the measured data 
  1. Model selection
  1. Sensitivity analysis
  1. Data checking
  1. Reporting
  1. Distribution of code and data.
#+end_src
** 2.5 implement
*** Rmd
#+begin_src R :session *R* :tangle swish-dmp-report.Rmd :exports none :eval no
  
  
  
  ## Planning and implementing a pipeline
  
  It can be much easier to conceptualise a complicated data analysis
  method than to implement this as a reproducible research pipeline. The
  most effective way to implement a pipeline is by methodically tracking
  each of the steps taken, the data inputs needed and all the outputs of
  the step.  If done in a disciplined way then the analyst or some other
  person could 'audit' the procedure easily and access the details of
  the pipeline they need to scrutinise.
  
  ### A standardised data analysis pipeline framework
  
  One method that was selected for use in the papers of this thesis was
  the concept of the Load-Clean-Functions-Do (LCFD) framework.  This was
  first proposed by Josh Reich on the open-source software discussion
  forum called 'stack overflow' (http://stackoverflow.com/a/1434424),
  and then encoded into the 'makeProject' R package
  (http://cran.r-project.org/web/packages/makeProject/makeProject.pdf).  The approach is demonstrated in case study 2 below.
  
  \clearpage

  ### Case study 2: Simple pipeline using the makeProject package 
  ```{r, eval = F}
  # in an interactive R session at the command line choose your project directory
  setwd("~/projects")   
  # load the required functions from the makeProject package
  library(makeProject)
  # use the makeProject function to 
  makeProject("my_first_pipelines_project")
  
  ### gives
  /my_first_pipelines_project/
      /code/*.R
      /data/
      /DESCRIPTION
      /main.R
  
  # in main.R you put these lines into the script and run them as the steps of the pipeline evolve
  source("code/load.R")
  source("code/clean.R")
  source("code/func.R")
  source("code/do.R")

  # Reporting is then a matter of choice
  ## If using the rmarkdown approach there would be an Rmd file that contained the prose
  ## and turned into a PDF, HTML or Word document with a line such as 
  rmarkdown::render("My-Pipeline-Report.Rmd", "pdf_document")
  ```
#+end_src
*** COMMENT snip
#+name:snip
#+begin_src R :session *R* :tangle no :exports none :eval no
  ### More complicated pipeline framework for data analysis
  - http://projecttemplate.net
  
  ```{r, eval = F}
  /project/
      /cache/
      /config/
      /data/
      /diagnostics/
      /doc/
      /graphs/
      /lib/
          /helpers.R
      /logs/
      /munge/
      /profiling/
          /01_profile.R
      /reports/
      /src/
          /01_EDA.R
          /02_clean.R
          /03_do.R
      /tests/
          /01_tests.R
      /README
      /TODO
  ```
  
  
  ### An example of a more personalised pipeline  
  - Long 2008 wrote a book recommended folder structure for statistical programmers 
  - http://www.indiana.edu/~jslsoc/web_workflow/wf_home.html
  - Recently updated with Long, S. (2015). Workflow for Reproducible Results. IV : Managing digital assets Workflow for Tools for your WF. http://txrdc.tamu.edu/documents/WFtxcrdc2014_4-digital.pdf
  
  ```{r, eval = F}
  \ProjectAcronym
      \- History starting YYYY-MM-DD
      \- Hold then delete 
      \Admin
      \Documentation 
      \Posted
           \Paper 1
               \Correspondence 
               \Text
               \Analysis
      \PrePosted 
      \Resources 
      \Write 
      \Work
  ```
#+end_src

*** COMMENT files files files

#+begin_src R :session *R* :tangle swish-dmp-report.Rmd :exports none :eval no    
  ### File organization and naming
  
  In many stages of a pipeline, an analyst will want to include details
  of the settings or what dataset they started out with. Rather than
  saving a folder or file name that is long and uninformative there are
  many different ways to organizing folders and files.
  
  Key techniques for this are available and known in the data analysis
  community as 'Tidy Data' guidelines.  In the words of 
  `r citet(bib[["WickhamRstudio2014"]])` the order that data should be
  arranged in follows some generic principles:
  
  \begin{quote}
  'A good ordering makes it easier to scan the raw values. One way of
  organizing variables is by their role in the analysis: are values
  fixed by the design of the data collection, or are they measured
  during the course of the experiment? Fixed variables describe the
  experimental design and are known in advance. Computer scientists
  often call fixed variables dimensions, and statisticians usually
  denote them with subscripts on random variables. Measured variables
  are what we actually measure in the study. Fixed variables should come
  first, followed by measured variables, each ordered so that related
  variables are contiguous. Rows can then be ordered by the first
  variable, breaking ties with the second and subsequent (fixed)
  variables.'
  \end{quote}
  
  ### An exemplar
  
  The following protocol was developed for an ecology and biodiversity database that the author of this PhD thesis was involved with.  The naming convention relied heavily on a sequence of information being used to order the names of folders, subfolders and files.  This is:
  
  1. The project name (and optional sub-project name)
  1. Data type (such as experimental unit, observational unit, and/or measurement methods)
  1. Geographic location (State, Country)
  1. Temporal frequency and coverage (such as annual or seasonal tranches).
  
  ### The concepts of slow moving dimensions and fast moving variables
  
  The concept of dimensions and variables can be useful here, and especially for deciding on filenames.  Dimensions are fixed or change slowly while variables change more quickly.  By 'change', this  means that there are more of them. For example the project name is 'fixed', that is it does not change across the files, but the sub-project name does change, just more slowly (say there may be 2-3 different sub-projects within a project). Then there may be a set of data types, and these 'change' more quickly than the sub-project name.  Then the geographic and temporal variables might change quickest of all.
  
  So a general rule for the order of things can be stated. The fixed and slowly changing variables should come first (those things that don't change, or don't change much), 
  followed by the more fluid variables (or things that change more across the project). 
  List elements can then be ordered so that the groups of things that are similar will always be contiguous, and vary sequentially within clusters.
  
  An example is shown in Table \ref{tab:TableFiles} to describe this and make it easier to understand.  Here is a set of file names that were constructed for an ecological field sites project that sampled plot-based measurement locations.  At the begining of the procedure a controlled vocabulary of data types and their acronyms was created.
  
  
#+end_src
*** COMMENT snip
#+name:snip
#+begin_src R :session *R* :tangle swish-dmp-report.Rmd :exports none :eval no
  
  ```{r results='asis', echo=F}
  library(stringr)
  steps <- read.table(textConnection(
  "Filename                                                           \t Title                                                                                                                                 
  asn_fnqr_soil_charact_robson_2011.csv                               \tSoil Data, Far North Queensland Rainforest SuperSite, Robson Creek, 2011                                                             
  asn_fnqr_soil_pit_robson_2012.csv                                   \tSoil Pit Data, Water Content and Temperature, Far North Queensland Rainforest SuperSite, Robson Creek, 2012                          
  asn_fnqr_veg_seedling_robson_2010-2012.csv                          \tSeedling Survey,  Far North Queensland Rainforest SuperSite, Robson Creek, 2010-2012                                                 
  asn_fnqr_veg_seedling_transect_coord_robson_2010-2012.csv           \tSeedling Survey,  Far North Queensland Rainforest SuperSite, Robson Creek, 2010-2012                                                 
  asn_fnqr_core_1ha_robson_2014.csv                                   \tSoil Pit Data, Soil Characterisation, Far North Queensland Rainforest SuperSite, Robson Creek, Core 1 ha plot, 2014                  
  asn_fnqr_fauna_biodiversity_ctbcc_2012.csv                          \tVertebrate Fauna Biodiversity Monitoring, Far North Queensland Rainforest SuperSite, CTBCC, 2012                                     
  asn_fnqr_fauna_biodiversity_ctbcc_2013.csv                          \tVertebrate Fauna Biodiversity Monitoring, Far North Queensland Rainforest SuperSite, CTBCC, 2013                                     
  asn_fnqr_fauna_biodiversity_ctbcc_capetrib_2014.csv                 \tAvifauna Monitoring, Far North Queensland Rainforest SuperSite, Cape Tribulation, 2014                                               
  asn_fnqr_fauna_biodiversity_ctbcc_lu11a_2014.csv                    \tVertebrate Fauna Biodiversity Monitoring, Far North Queensland Rainforest SuperSite, CTBCC, LU11A, 2014                              
  asn_fnqr_fauna_biodiversity_ctbcc_lu7a_2014.csv                     \tVertebrate Fauna Biodiversity Monitoring, Far North Queensland Rainforest SuperSite, CTBCC, LU7A, 2014                               
  asn_fnqr_fauna_biodiversity_ctbcc_lu7b_2014.csv                     \tVertebrate Fauna Biodiversity Monitoring, Far North Queensland Rainforest SuperSite, CTBCC, LU7B, 2014                               
  asn_fnqr_fauna_biodiversity_ctbcc_lu9a_2014.csv                     \tVertebrate Fauna Biodiversity Monitoring, Far North Queensland Rainforest SuperSite, CTBCC, LU9A, 2014                               
  asn_fnqr_fauna_biodiversity_ctbcc-lu11a_2009-2011.csv               \tVertebrate Fauna Biodiversity Monitoring, Far North Queensland Rainforest SuperSite, CTBCC, LU11A, 2009-2011                         
  asn_fnqr_fauna_biodiversity_ctbcc-lu7a_2009-2011.csv                \tVertebrate Fauna Biodiversity Monitoring, Far North Queensland Rainforest SuperSite, CTBCC, LU7A, 2009-2011                          
  asn_fnqr_fauna_biodiversity_ctbcc-lu9a_2009-2011.csv                \tVertebrate Fauna Biodiversity Monitoring, Far North Queensland Rainforest SuperSite, CTBCC, LU9A, 2009-2011                          
  asn_fnqr_fauna_biodiversity_habitat codes_ctbcc-lu11a_2009-2011.pdf \tVertebrate Fauna Biodiversity Monitoring, Far North Queensland Rainforest SuperSite, CTBCC, LU11A, 2009-2011                         
  "), strip.white = T, sep = "\t", stringsAsFactor = F, header = T)
  #steps
  
  tabcode <- xtable(steps, caption = 'An example of standardised filename conventions to simplify tracking complicated datasets', label = 'tab:TableFiles')
  align(tabcode) <-  c( 'l', 'p{3in}','p{3in}' )
  #sink(paste(fname, '.tex',sep = ""))
  #cat(txt)
  print(tabcode,  include.rownames = F, table.placement = '!h',
   caption.placement = 'top', comment = F) #, type = "html")
  
  ```
  
  \clearpage
#+end_src
*** snip-code

### Key issues

- How to keep data organized and easy to reuse at a later date (including in-house reuse)
- Data storage/archiving! It's frightening how hard it can be to find raw data from a project - or if you can find it at all?
- How to organize your data with informative naming scheme and minimize duplication or lots of sub-dir
- Maybe a data citation unit such as Digital Object Identifier (DOI) that can provide links to data, along with good names and folder layout
- Concern is how to deal with in house data before you start your analysis, this is critical for reproducible research - so that you could go back and reproduce your analysis
- Recommended by some is 'convention over configuration' advice, and links to evidence based recommended filing systems. 

** 2.6 viz
*** TODO make better fit on A4 sheet, explain each of the main pathways in this pipeline, link to web version, put table in appendix
*** vis
#+begin_src R :session *R* :tangle swish-dmp-report.Rmd :exports none :eval no
  
  
  
  ## Visualisation techniques
  
  ### Make a list of steps, inputs and outputs
  
  A very simple example of a pipeline is shown in Table
  \ref{tab:TablePipe1}.  The steps and data listed in Table
  \ref{tab:TablePipe1} can be visualised using the `newnode` function
  described above in case study 1.  This creates the graph of this pipeline
  shown in Figure \ref{fig:FigSteps}.  As the analysis progresses
  through the phases of testing, refinement and final versions. The
  linked table and graphical depiction can be very helpful for reference
  by the analyst.  The optional setting to define a status of each step
  (TODO, DONE, WONTDO) can be used to add colour, and show steps that
  remain to be done.  The addition of short summary descriptions are
  also very useful for orienting oneself to the required tasks and their
  priorities.  Such flow chart diagrams can be printed up on large
  sheets of paper and stuck on the wall beside a computer workstation
  for use in day-to-day work.
  
  ```{r TablePipe1, results='asis', echo=FALSE}
  #, tab.cap="This is the first example table\\label{tab:Table1}",cache=FALSE}
  library(stringr)
  steps <- read.csv(textConnection('
  CLUSTER ,  STEP    , INPUTS                  , OUTPUTS                                , DESCRIPTION                        , STATUS 
  A  ,  Step1      , "Source 1, Source 2"       , "Derived 1, QC check"                 , "This might be latitude and longitude of sites"     ,  DONE
  A  ,  Step2      , Source 3                  , Derived 2                           , This might be weather data               , DONE
  B  ,  Step3      , "Derived 1, Derived 2"      , Derived 3                             , Merging these data means they can be analysed   , TODO
  C  ,  Step4      , Derived 3                 , Model selection                              ,                                    , TODO
  C  ,  Step5      , Model selection           , Sensitivity analysis                         ,                                    , TODO
  '), stringsAsFactors = F, strip.white = T)
  
  #kable(
  dat <- steps[,c("STEP", "INPUTS", "OUTPUTS", "DESCRIPTION", "STATUS")]
  library(xtable)
  tabcode <- xtable(dat, caption = 'A table with the steps of a simple data analysis pipeline', label = 'tab:TablePipe1')
  align(tabcode) <-  c( 'l', 'p{.6in}','p{1.3in}','p{1.2in}', 'p{2in}','p{1in}' )
  #sink(paste(fname, '.tex',sep = ""))
  #cat(txt)
  print(tabcode,  include.rownames = F, table.placement = '!ht',
   caption.placement = 'top', comment = F) #, type = "html")
  #rws <- seq(1, (nrow(dat)), by = 2)
  #col <- rep("\\rowcolor[gray]{0.95}", length(rws))
  #print(tabcode, booktabs = TRUE, include.rownames = F, table.placement = '!ht',
  # caption.placement = 'top',
  # add.to.row = list(pos = as.list(rws), command = col),
  # comment=FALSE)
  
  ```
  
  
  
  ```{r echo=F, eval=F}
  library(disentangle); library(stringr)
  nodes <- newnode(indat = steps,   names_col = "STEP", in_col = "INPUTS",
    out_col = "OUTPUTS", desc_col = 'DESCRIPTION',
    nchar_to_snip = 70)
  #, todo_col = "STATUS",
  sink("thesis/steps-fig1.dot"); cat(nodes); sink()
  #DiagrammeR::grViz("steps-fig1.dot")
  system("dot -Tpdf thesis/steps-fig1.dot -o vignettes/steps-fig1.pdf")
  
  ```
  
  
  
  \begin{figure}[!h]
  \centering
  \includegraphics[width=\textwidth]{steps-fig1.pdf}
  \caption{A visualisation of a data analysis pipeline showing the use of colour}
  \label{fig:FigSteps}
  \end{figure}
  
  \clearpage
  
  
  
  As an example of the kinds of tangible steps such a workflow might
  entail a schematic diagram has been created and is shown in Figure \ref{fig:envepi_data_pipeline.png}.
  
  
  
  \begin{figure}[!h]
  \centering
  \includegraphics[width=\textwidth]{envepi_data_pipeline.pdf}
  \caption{A schematic flow chart showing the steps required to prepare and
  conduct an analysis of health, environmental and social data.}       
  \label{fig:envepi_data_pipeline.png}
  \end{figure}
  

#+end_src
** refs
#+begin_src R :session *R* :tangle swish-dmp-report.Rmd :exports none :eval no

  # References
  
  ```{r, echo=FALSE, message=FALSE, eval = T}
  write.bibtex(file="references.bib")
  ```
  
  
#+end_src
