#+TITLE: Data management procedures for reproducible research pipelines 
#+AUTHOR: Ivan Hanigan
#+email: ivan.hanigan@anu.edu.au
#+LaTeX_CLASS: article
#+LaTeX_CLASS_OPTIONS: [a4paper]
#+LATEX: \tableofcontents
-----
* workplan
- phd appendix is not as polished as I would like, so leave that as a side branch
- future development will move forward on master branch
* worklog
** TODO 2016-01-26 note re: automating boilerplate EML for wide data
*** COMMENT go
#+name:go
#+begin_src R :session *R* :tangle no :exports none :eval yes
#### name:go ####
  setwd("~/projects/swish-dmp/notes")
  rmarkdown::render("automating_boilerplate_EML_for_wide_data.Rmd", "html_document")  
  #browseURL("automating_boilerplate_EML_for_wide_data.html")

#+end_src

#+RESULTS: go
: /home/ivan_hanigan/projects/swish-dmp/notes/automating_boilerplate_EML_for_wide_data.html

*** COMMENT notes/automating_boilerplate_EML_for_wide_data
#+name:notes/automating_boilerplate_EML_for_wide_data
#+begin_src R :session *R* :tangle notes/automating_boilerplate_EML_for_wide_data.Rmd :exports none :eval no
  ---
  title: "notes re automating_boilerplate_EML_for_wide_data"
  ---
  
  # Introduction
  I am re-opening this pull request, more as a 'request-for-comment' than as an additional feature of the EML package.  I don't have the R packaging/travis etc skills (or the time to develop them) to adequately meet the requirements for code contributions.  So I thought I'd write to you and describe the function I was trying to develop, and how I used it when publishing many wide datasets to a metacat portal.
  
  Some time ago I wrote a function that automated the steps needed for entering repetitive metadata when one has a 'wide' dataset.  
  
  
  I was trying to address the issue you refer to in the script [https://github.com/ropensci/EML/blob/master/manuscripts/one-simple-way.Rmd](https://github.com/ropensci/EML/blob/master/manuscripts/one-simple-way.Rmd), and I would like to contribute some information that relates to the following point, primarily around Line 234: [https://github.com/ropensci/EML/blame/master/manuscripts/one-simple-way.Rmd#L234](https://github.com/ropensci/EML/blame/master/manuscripts/one-simple-way.Rmd#L234)
  
  Firstly I believe there is a typo in this section and you mean to say 'observations for a single site added as additional _columns_' rather than '_rows_' as it currently reads.  More rows would make it long. I am pretty sure you mean to imply that researchers add columns for new observations from a site.
  
  
  ```
  many researchers are still tempted to present data in "wide" format,
  with observations for a single site added as additional rows (sic). While
  potentially convenient for field entry, the long format is more
  desirable for most future analysis or reuse.
  
  The EML package addresses this by both working in the R an environment
  where it is easy and routine to transform between "long" and "wide"
  formats, while also giving the user the flexibility to simply document
  the data in whatever format they feel works best for their analysis.
  ```
  
  
  Second, I would like to explore the claim that the EML package addresses this issue by a) 'working in R where it is easy to transform' and b) also 'gives flexibility to document the data in whatever format they feel works best'.  I would like to explore these two propositions using some data that provided the motivating use case for me to develop the function mentioned above, and ask for your advice on whether this was a sensible approach, and if so what I might do with it now.
  
  ## The use case example: Fire with season and frequency 
  
  In the example of using my function I would like to show one of the datasets that led me to develop the function in the first place.  It comes from the following data package:
  
  Russell-Smith, J; Darwin Centre for Bushfires Research, Charles Darwin
  University; Director of National Parks (Parks Australia); Parks and
  Wildlife Commission of the Northern Territory (2015): Three Parks
  Savanna Fire-effects Plot Network: Plot-based Fire Severity
  Associations with Season and Frequency, Kakadu, Litchfield and
  Nitmiluk National Parks, Northern Territory, Australia,
  1994â€“2013. Long Term Ecological Research
  Network. [http://www.ltern.org.au/knb/metacat/ltern.102/html](http://www.ltern.org.au/knb/metacat/ltern.102/html). Accessed
  on 26/1/2016.  
  
  Having downloaded the data (they require you to log on first) you can see that there is one long dataset and one wide dataset.  The source data are kept in wide format for precisely the reason you state above, namely that the researchers find it preferable to add a new column after each field season rather than to convert their data to long format.  The issue arose when these data were provided for publication at the data portal and the structure of this file made preparation of metadata difficult in the `morpho` software (by which I mean slow and error-prone).  The following code chunk shows the use of my function and your EML package to quickly generate the required EML for each column in this spreadsheet.
#+end_src
*** COMMENT code
#+name:notes/automating_boilerplate_EML_for_wide_data
#+begin_src R :session *R* :tangle notes/automating_boilerplate_EML_for_wide_data.Rmd :exports none :eval no
  
  ```{r}
  projdir <- "~/projects/Biodiversity_and_environmental_change_LTERN/fire_season_and_frequency/"
  setwd(projdir)
  indir <- "data_provided"
  infile <- "tps_fire_severity_p366_combined_qc_reshape_wide.csv"
  dat <- read.csv(file.path(indir, infile), as.is = T)
  ncol(dat)
  # 57 columns in morpho is a pain, and also error prone
  # just show the first 5 and last 5 columns to get a feel for the data
  str(dat[,c(1:5)])
  ## 'data.frame':        219 obs. of  5 variables:
  ##  $ plot       : chr  "KAKFIRE_1" "KAKFIRE_10" "KAKFIRE_100" "KAKFIRE_11" ...
  ##  $ x1994_a_wet: int  0 0 0 0 0 0 0 0 0 0 ...
  ##  $ x1995_a_wet: chr  "0" "0" "0" "0" ...
  ##  $ x1995_b_eds: chr  "0" "1" "0" "0" ...
  ##  $ x1995_c_lds: chr  "0" "0" "0" "0" ...
  
  str(dat[, c((ncol(dat) - 5):ncol(dat))])
  ## 'data.frame':        219 obs. of  6 variables:
  ##  $ x2011_c_lds: chr  NA NA NA NA ...
  ##  $ x2012_a_wet: int  NA NA NA NA NA NA NA NA NA NA ...
  ##  $ x2012_b_eds: chr  NA "U" "U" "2" ...
  ##  $ x2012_c_lds: chr  NA NA "U" "U" ...
  ##  $ x2013_b_eds: chr  NA NA NA NA ...
  ##  $ x2013_c_lds: chr  NA NA NA NA ...
  
  
  # the function I wrote to automate this
  library(downloader)
  download("https://raw.githubusercontent.com/ivanhanigan/EML/devel/R/eml_boilerplate.R",
    "eml_boilerplate.R")
  source("eml_boilerplate.R")
  library(EML)
  
  outfile <- file.path("data_derived", gsub(".csv", "2.csv", infile))
  outeml <- file.path("data_derived", gsub(".csv", "2.xml", infile))
  
  unit_defs <- eml_boilerplate(dat, enumerated = NA)
  col_defs <- names(dat)
  ds <- eml_dataTable(dat,
                col.defs = col_defs,
                unit.defs = unit_defs,
                description = "TBA", 
                filename = outfile)
  # now write EML metadata file
  eml_config(creator="TBA")
  eml_write(ds,
            file = outeml,
            title = "TBA"
  )
  ```
  
  This has produced an EML with the columns entered as numeric or character depending on how the CSV was imported, as we saw above 2012_a_wet was seen as numeric whereas 2012_b_eds was seen as text.  The function also recognises if a variable is stored as a date format.
  
  ```
          <attribute>
            <attributeName>x2012_a_wet</attributeName>
            <attributeDefinition>x2012_a_wet</attributeDefinition>
            <measurementScale>
              <ratio>
                <unit>
                  <standardUnit>number</standardUnit>
                </unit>
                <numericDomain>
                  <numberType>real</numberType>
                </numericDomain>
              </ratio>
            </measurementScale>
          </attribute>
          <attribute>
            <attributeName>x2012_b_eds</attributeName>
            <attributeDefinition>x2012_b_eds</attributeDefinition>
            <measurementScale>
              <nominal>
                <nonNumericDomain>
                  <textDomain>
                    <definition>TBA</definition>
                  </textDomain>
                </nonNumericDomain>
              </nominal>
            </measurementScale>
          </attribute>
  
  ```
  
  So that was quick and dirty.  What I needed to also do was ensure that all columns are character (because in seasons where only the fire severity score '1' is observed, this is imported as numeric), and running this again I can also specify that all columns after the first one are enumerated values which will also write out the levels of each:
  
  ```
  # convert to character
  for(i in 2:57){
    dat[,i] <- as.character(dat[,i])
  }
  # create unit defs, but look at the levels of the enumerated values too
  unit_defs <- eml_boilerplate(dat, enumerated = 2:57)
  col_defs <- names(dat)
  ds <- eml_dataTable(dat,
                col.defs = col_defs,
                unit.defs = unit_defs,
                description = "TBA", 
                filename = outfile)
  # now write EML metadata file
  eml_config(creator="TBA")
  eml_write(ds,
            file = outeml,
            title = "TBA"
  )
  
  ```
  
  This achieves the following XML which is pretty close to what we wanted:
  
  ```
          <attribute>
            <attributeName>x2012_a_wet</attributeName>
            <attributeDefinition>x2012_a_wet</attributeDefinition>
            <measurementScale>
              <nominal>
                <nonNumericDomain>
                  <textDomain>
                    <definition>TBA</definition>
                  </textDomain>
                </nonNumericDomain>
              </nominal>
            </measurementScale>
          </attribute>
          <attribute>
            <attributeName>x2012_b_eds</attributeName>
            <attributeDefinition>x2012_b_eds</attributeDefinition>
            <measurementScale>
              <nominal>
                <nonNumericDomain>
                  <enumeratedDomain>
                    <codeDefinition>
                      <code>x1</code>
                      <definition>TBA</definition>
                    </codeDefinition>
                    <codeDefinition>
                      <code>x2</code>
                      <definition>TBA</definition>
                    </codeDefinition>
                    <codeDefinition>
                      <code>x3</code>
                      <definition>TBA</definition>
                    </codeDefinition>
                    <codeDefinition>
                      <code>u</code>
                      <definition>TBA</definition>
                    </codeDefinition>
                  </enumeratedDomain>
                </nonNumericDomain>
              </nominal>
            </measurementScale>
          </attribute>
  ```  
  
  Well, I actually wanted it to define the value of '2' for the 2012_a_wet variable, but you can see it has extracted the levels of fire severity in 2012_b_eds.  However this is not exactly what we want because in this specific case it would be more desirable to explicitly describe every possible level of the enumerated values, even in seasons where not every fire type was observed.  We achieved this by constructing a dummy dataset where every column contained an observation with every possible value from the fire severity index.  And then swapping the correct data file back in when uploading the EML to metacat.  It would have been nice to have allowed for some additionaly inputs by the user to specify not only which columns are enumerated values, but also the full extent of possible observations, rather than just extract the levels that were actually observed.
  
  ## Results
  
  This has achieved my main goal of speeding up the creation of EML for wide formatted data to meet the desire of the researcher to keep the data in this format.  I think this addresses the point (B) above that the EML package gives the flexibility to create of the EML for whatever format the researcher feels works best, and my function is a lot faster and less error-prone than morpho.
  
  Point (A) above is of course true and if you click through to the published data at [http://www.ltern.org.au/knb/metacat/ltern.102/html](http://www.ltern.org.au/knb/metacat/ltern.102/html) you will see we also published the long version of these data (with the addition of a variable for date and also for time since last file), having used the same 'boilerplate' function in the following way:
  
  ```
  library(reshape)
  dat2 <- melt(dat, c("plot"))
  str(dat2)
  ## 'data.frame':        12264 obs. of  3 variables:
  ##  $ plot    : chr  "KAKFIRE_1" "KAKFIRE_10" "KAKFIRE_100" "KAKFIRE_11" ...
  ##  $ variable: Factor w/ 56 levels "x1994_a_wet",..: 1 1 1 1 1 1 1 1 1 1 ...
  ##  $ value   : Factor w/ 6 levels "0","1","2","B",..: 1 1 1 1 1 1 1 1 1 1 ...
  
  # My function is also quite handy even with long data because it saves a bit of typing
  unit_defs <- eml_boilerplate(dat2, enumerated = 3)
  unit_defs
  ## $plot
  ## [1] "TBA"
  
  ## $variable
  ## [1] "TBA"
  
  ## $value
  ##    x0    x1    x2     b     u    x3 
  ## "TBA" "TBA" "TBA" "TBA" "TBA" "TBA" 
   
  ```
    
  
  ## Future directions?
  
  - I no longer work at the Australian LTER plot networks data portal, so the motivating use case of quickly producing EML has reduced.  I still work in the area of integrating and synthesising heterogenous datasets (for eco-epidemiology now) so I would still like to build my tool kit around the EML package.
  - But I don't want to develop this function any further in the near future.
  - I think it is a useful tool though, but is probably not destined to be part of the core features of the EML package. Perhaps it might be available as an unstable EML-helper or add-in?  
  - I would value your advice and comments about the approach I have taken and what I might do with this stuff in the future.
  
#+end_src



* PhD appendix (make sure you are on PhD branch in subfolder)
** Run the Rmd
#+begin_src R :session **R** :tangle no :exports none :eval yes :padline yes
  
  # func
  setwd("~/projects/swish-dmp/swish_data_management_procedures")
  library(rmarkdown)
  library(knitr)
  library(knitcitations)
  library(bibtex)
  cleanbib()
  # rm("bib")
  #options("cite_format"="pandoc")
  cite_options(citation_format = "pandoc", check.entries=FALSE)
  
  #rmarkdown::render("swish-dmp-report.Rmd", "html_document")  
  rmarkdown::render("swish-dmp-report.Rmd", "pdf_document")
  # browseURL("swish-dmp-report.pdf")
  # browseURL("swish-dmp-report.html")
#+end_src  

#+RESULTS:
: /home/ivan_hanigan/projects/swish-dmp/swish_data_management_procedures/swish-dmp-report.pdf

** head

#+begin_src R :session **R** :tangle swish_data_management_procedures/swish-dmp-report.Rmd :exports none :eval no
  ---
  title: "Data management procedures for reproducible research pipelines"
  author:  
  - name: Ivan C. Hanigan
    email: ivan.hanigan@anu.edu.au  
  header-includes:
    - \usepackage{graphicx}
    - \usepackage{url}   
  output:
    pdf_document:
      fig_caption: yes
      keep_tex: yes
      number_sections: yes
      template: components/manuscript.latex
      toc: yes
    html_document: null
    word_document: null
  fontsize: 11pt
  capsize: normalsize
  csl: components/meemodified.csl
  documentclass: article
  classoption: a4paper
  spacing: singlespacing
  linenumbers: no
  bibliography: references.bib
  abstract:   This unpublished working paper was written to  accompany the material included in the PhD thesis 'Using Reproducible Research Pipelines to Help Disentangle Health Effects of Environmental Changes from Social Factors' by Ivan Hanigan (2016).  It sets out the key data management and analysis principles that were found to be most effective for the reproducibile synthesis and integration of heterogeneous datasets for analysis and reporting.  The draft was last updated \today. The version submitted with the thesis is available as part of a Github repository at [https://github.com/swish-climate-impact-assessment/swish_data_management_procedures/blob/phd_appendix/swish-dmp-report.pdf](https://github.com/swish-climate-impact-assessment/swish_data_management_procedures/blob/phd_appendix/swish-dmp-report.pdf).
  
  ---
  
  ```{r echo = F, eval=F, results="hide"}
  # func
  setwd("~/projects/swish-dmp/report1_appendix_for_phd")
  library(rmarkdown)
  library(knitr)
  library(knitcitations)
  library(bibtex)
  cleanbib()
  # rm("bib")
  #options("cite_format"="pandoc")
  cite_options(citation_format = "pandoc", check.entries=FALSE)
  
  #rmarkdown::render("swish-dmp-report.Rmd", "html_document")  
  rmarkdown::render("swish-dmp-report.Rmd", "pdf_document")
  # browseURL("swish-dmp-report.pdf")
  # browseURL("swish-dmp-report.html")
  
  ```
  ```{r echo = F, results="hide"}
  # load
  if(!exists("bib")){
  bib <- read.bibtex("~/references/library.bib")
   
  for(bibkey in c("SarathiBiswas2012",
    "Mcmichael2002a", "Gelman2013"
  )){
  bib[[bibkey]]$url <- gsub("\\{\\\\_\\}","_", bib[[bibkey]]$url)
  bib[[bibkey]]$url <- gsub("\\{~\\}","~", bib[[bibkey]]$url)
  }
   
  }
  ```
  \clearpage
  \doublespace
  
#+end_src
  
** Introduction Rmd
#+begin_src R :session **R** :tangle swish_data_management_procedures/swish-dmp-report.Rmd :exports none :eval no
  
  # Introduction
  
  There is a need for developing an evidence based set of best practice
  guidelines for data management procedures in implementing reproducible
  research pipelines in epidemiology `r citep(bib[["Peng"]])`.  The
  examples drawn together in this report come from experiences and
  use-cases found in an eco-social epidemiologic research context.  The
  emerging paradigm of eco-social epidemiology is inherently concerned
  with complex systems and integrating heterogenous data sources to aid
  recognition of patterns in the environmental and social determinants
  of population health `r citep(bib[["McMichael2013"]])`.  This document
  outlines a suite of data management procedures that have been found to
  effectively assist the development of reproducible research pipelines.
#+end_src
** snip-intro
## Introduction to reproducible epidemiological data science
  The details of the scientific questions that motivated the work on
  reproducibility are not included, however throughout the discussion
  there is the imperative that the techniques suite the
  context of environmental epidemiology.  This requirement guided
  the selection of the material included.  Other scientific disciplines
  may find different components of pipelines are required for specific
  data collection and management techniques. 
** Configuration versus convention: the case for standardised approaches  
#+begin_src R :session **R** :tangle swish_data_management_procedures/swish-dmp-report.Rmd :exports none :eval no
  
  # Configuration versus convention: The case for standardised approaches  
  
  Reproducibility is the ability to recompute the results of a data
  analysis with the original data.  It is possible to have analyses that
  are reproducible with varying degrees of difficulty. A data
  analysis might be reproducible but require thousands of hours of work.
  A primary challenge for reproducible data analysis is to make analyses
  that are \emph{easy} to reproduce.
  
  In essence this requires attention to be turned to the issue of how
  the data and analytical steps amassed â€“ toward a reality where this is
  archived and there is a good understanding all round as to how the
  study were set up and conducted.  Different assumptions or different
  treatment of the data could conceivably lead to different inferences
  and conclusions being drawn, such as in the example shown by `r citet(bib[["Silberzahn2015"]])` in which 29 research
  teams were given the same dataset but reached a wide variety of
  conclusions using different methods on the same dataset to answer the
  same question.
  
  This is partly because of an underlying complexity in the information
  drawn from complex systems involving multi-causality, and partly
  because of different assumptions and different backgrounds and
  viewpoints. A finding that a variable does or does not cause a
  disease, might be drawn honestly from the same set of data.
  
  The techniques of pipelines described here are targeting the integrity
  of the process of data selection, the robustness and suitableness of
  the methods used, a commonsense and well-argued selection of health
  outcomes and environmental or social exposures, and the clarity and
  transparency of the methods used.
  
  To achieve this, a guiding principle is that analysts should
  effectively implement 'pipelines' of method steps and tools.
  Standardised and evidence-based methods based on conventions developed
  from many data analysts approaching the problems in a similar way
  should be used, rather than each analyst configuring a pipeline to
  suit particular individual or domain-specific preferences  `r citep(c(bib[["Borer2009a"]],bib[["White2013b"]]))`.
  
  `r citet(bib[["Noble2009"]])` points out that
  'the principles behind organizing and documenting computational experiments are often learned on the fly, and this learning is strongly influenced by personal predilections'.
  `r citet(bib[["Leek2015b"]])` describe this as data analysis being
  'taught through an apprenticeship model, and different disciplines develop their own analysis subcultures'. By
  codifying what an appropriate pipeline would contain, data analysis
  will be more robust.  According to `r citet(bib[["Peng"]])`, there
  should not be a 'lonely data analyst' coming up with their own
  method. If a researcher conducted an analysis using an evidence-based reproducible research
  pipeline
  'you could at least have a sense that something reasonable was done'
  and be confident that you could easily check what had been done if you needed to.
#+end_src
** Core components of pipeline
#+begin_src R :session **R** :tangle swish_data_management_procedures/swish-dmp-report.Rmd :exports none :eval no
  ## The core components of a pipeline
  As mentioned in chapter 1, `r citet(bib[["Peng2006"]])` distilled a core set of components for reproducibility from earlier work including that of `r citet(bib[["Schwab2000"]])`.  These are:
  
  - Hypothesis and design
  - Data (measurement, pre-processing, analytic)
  - Analysis Methods
  - Documentation (of all steps)
  - Distribution (of the paper, data and code).
   
  
#+end_src
** hypothesis and design
#+begin_src R :session **R** :tangle swish_data_management_procedures/swish-dmp-report.Rmd :exports none :eval no
  
  ## Hypothesis and design
  
  The first stage of the pipeline is hypothesis generation and study
  design.  In this stage documentation should explain the literature
  base supporting the study, the decisions made in selection of explanatory
  factors for inclusion, decisions made such as the experimental unit,
  observational unit, measurement method, as well as spatial or temporal
  extent.  This information will also be needed for ethical review and approval.
#+end_src
** data
#+begin_src R :session **R** :tangle swish_data_management_procedures/swish-dmp-report.Rmd :exports none :eval no
  
  ## Data
  
  The data that were measured should be well managed, however the
  requirements for accessing the original raw data are less important
  than for the analytical dataset.  Descriptions of how the measured
  data were transformed into the analytic data should be available.
  Public data repositories or institutional services such as university
  libraries should be used to ensure longevity of the data storage.  
#+end_src
** methods
#+begin_src R :session **R** :tangle swish_data_management_procedures/swish-dmp-report.Rmd :exports none :eval no
  
  ## Methods
  
  The software code underlying the principal
  results needs to be made available. In
  addition, the computer environment necessary to execute that code should be
  described adequately to 'deploy' a new computer set-up
  that can reproduce the computations needed.
#+end_src
** documentation
#+begin_src R :session **R** :tangle swish_data_management_procedures/swish-dmp-report.Rmd :exports none :eval no
  
  ## Documentation
  
  Adequate documentation of the code and
  data should be available to enable others to repeat the
  analyses and to conduct other similar ones.  This can take the form of metadata, reports, journal papers or even books `r citep(bib[["Peng2008a"]])`. Indeed textbooks on statistical methods can benefit greatly from being accompanied by data and analytical code to enhance their pedagogic functions `r citep(c(bib[["Barnett2015"]], bib[["Barnett2010"]]))`.
  
  An important underpinning to reproducible research is the
  reproducible report.  This is the ultimate form of documentation
  because the information that represents the outputs of the research is
  written alongside the code that performs the computations that are
  being described.  There has been many recent advances made in terms of
  tools for reproducible reports such as R markdown and knitr `r citep(bib[["Xie2014a"]])`.
  
  Metadata should be created and maintained as a priority task at all
  stages of the data analysis process.  An international standard should
  be preferred over selectively choosing what information one collects
  and what fieldnames one uses to describe each item of documentation.
  Ecological Metadata Language (EML) and the Data Documentation
  Inititative (DDI) are two such standards that offer useful semantic
  constructs for describing epidemiological data.
#+end_src
** distribution
#+begin_src R :session **R** :tangle swish_data_management_procedures/swish-dmp-report.Rmd :exports none :eval no
  
  ## Distribution
  
  Distribution or dissemination of the material needs to use a standard
  method if they are to be used by others.  It is not enough just to
  provide access to the software and data, but also adequate
  documentation is required to explain and potentially assist downstream
  users to piece these together.
  
#+end_src

** snip distro
#+begin_src R :session **R** :tangle no :exports none :eval no
  ## Distributing data, code and documentation
  
  - This section is still TODO
  - Sharing computer code via Github
  - Publishing well documented data with metadata in a standard format (EML, DDI, ANZLIC)
#+end_src
** COMMENT snip rationale
#+name:snip
#+begin_src R :session **R** :tangle no :exports none :eval no
  #### name:snip ####

    
  However, in many cases it is hard or impossible to
  exactly replicate an epidemiological study: time moves on, the
  demographics change, the drought, smog or smoke conditions can\'t be
  replicated. Even so, in principle, one might
  reasonably conduct a similar study and obtain similar health results.
  In practice it has been found that, because of the time and expense
  constraints of many epidemiological studies, it is unrealistic to
  expect replication of findings. A pragmatic alternative has been
  proposed because it is recognised that a more attainable minimum
  standard is 'reproducibility', which calls for data sets and software
  to be made available for verifying published findings and conducting
  alternative analyses `r citep(bib[["Peng2006"]])`.  

  This model has been referred to in general as the reproducible
  research pipeline by `r citet(bib[["Solymos2008"]])` who created the graphical view shown in Figure \ref{fig:reproduciblepipeline}. This model is also sometimes called the 'data science pipeline'
  `r citep(bib[["Peng"]])` and is related to the practice of
  'evidence based data analysis' (where tools and techniques are applied
  based on knowledge of their effectiveness, not just on assumptions by
  the analyst).  


#+end_src

** procedures of a RRP 
#+begin_src R :session **R** :tangle swish_data_management_procedures/swish-dmp-report.Rmd :exports none :eval no
  
  
  
  # Procedures when conducting a reproducible research analysis
  
  Having defined above the principle components for a pipeline there are procedural questions about how to go about compiling those. The key steps include:
  
  - Data Management Plans and Data Inventories
  - Tracking method steps
  - Developing code
  - Maintaining data storage
  - Writing reports
  - Distributing the materials.

#+end_src
** Data management plan and data inventory
#+begin_src R :session **R** :tangle swish_data_management_procedures/swish-dmp-report.Rmd :exports none :eval no
    
  ## Data management plan and data inventory
  
  In eco-social epidemiology there is a need for a data management
  plan and a data inventory that enables individual scientists, or
  multidisciplinary teams of scientists, to manage large and
  heterogeneous collections of disparate data sources efficiently.
  Keeping track of all the elements of a linked health, social and
  environmental database is very challenging, despite major improvements
  in data management software, web-portals and virtual laboratories
  `r citep(bib[["Fleming2014" ]])`.
  
  Effective data management policies and procedures are essential in
  managing data-related risk. Such risks include data loss or
  corruption, technological obsolescence, breaches of privacy or
  copyright, and errors or misuse.  Misuse may be due either to
  unintended user misunderstandings about data attributes (no dataset is
  perfect and self-explanatory, see `r citet(bib[["Michener1997" ]])`)
  or intentional mis-use for malicious or selfish reasons (for example
  the misuse of data by Bjorn Lomborg to support the argument that
  environmental health conditions are actually improving.  See
  `r citet(bib[["Bodnar2004" ]])` for a discussion on Lomborg\'s misuse
  of data.  There have also been notable examples of mistakes in data
  analyses used for climate change science.  See
  `r citet(bib[["Cai2010" ]])` for a discussion of one such case.  The
  careful storage and curation of datasets is also critical because data from many
  studies are lost
  `r citep(c(bib[["Pullin2010" ]], bib[["Vines2014a" ]]))`.
  
  Data management plans are needed for developing procedures and
  processes to keep data safe.  There is an issue when ensuring that all
  relevant data are collected in deciding what is relevant.  Keeping an
  up-to-date data inventory and careful organisation of all folders and
  files helps mitigate these problems.
  
  Whether data management is the responsibility of the individuals
  collecting or collating it, or of the lead scientist, clarity on how
  and where data are stored and who manages it is vital, as is a
  'succession plan' that sets out the vision of the data collections
  preservation and re-use into the future.
  
  
  
  
#+end_src
** Case study 1: EML and folder structure
#+begin_src R :session **R** :tangle swish_data_management_procedures/swish-dmp-report.Rmd :exports none :eval no

    
  ## Case study 1: Ecological Metadata  Language (EML) and folder structure
  
  For data to be reused in the future, metadata and documentation need to be carefully
  prepared to allow future users (including the original collector) to find and understand the
  data (Michener et al. 1997). Metadata should be associated with the data
  and adhere to a standard schema. This example shows the use of the Ecological Metadata Language (EML). 
  
  Good metadata requires sufficient detail
  to describe the collection process and to record decisions that were made during the
  design phase about the use of different sampling methods. Time and effort may be saved
  by considering metadata requirements at the commencement of a study, rather than trying
  to recall all the details later. If metadata adheres to a standard schema, it can be used in
  catalogues to enable fast searching and retrieval, or in machine-to-machine data queries
  that assist data access and use.
    
  In EML the elements of any dataset can be seen as a nested hierarchy at three levels:
   
  1. The Project level: this is an overarching grouping of data.  It might be indicative of the principal investigator or organisation who provided the data, or a programme of research studies (sub-projects).
  2. The Dataset level: this is a distinct grouping of data that might be organised around a particular time period or geographical region.
  3. The Entity level: This grouping of data includes data files (such as tables in CSV or Excel, shapefiles and raster images) or documents (such as metadata descriptions or related publications). 
  
  This conceptual framework can be very useful for the organisation of the work constituting a single pipeline, as well as when working with multiple pipelines within several projects.
    
  ![images/EML_project.png](images/EML_project.png)
#+end_src


** data storage and access

#+begin_src R :session **R** :tangle swish_data_management_procedures/swish-dmp-report.Rmd :exports none :eval no
  ## Data storage and access 
  
  Some datasets such as sensitive personal information about suicide or
  climate change scenarios with restrictions due to privacy and
  confidentiality rules, or because of protected intellectual property,
  need to be accessed in a restricted way.  This complicates the
  implementation of the method of pipelines which dictates that all the
  steps, models and assumptions need to be made transparent and
  available for scientific debate even though the datasets may require
  authorisation to access.  Restrictions around access to data have
  increased recently in Australia.  As an example the custodians of the
  national mortality database made it virtually impossible to access
  these data for several years after the discovery of an incident in
  which Australian population health researcher Dr Stephen Begg was reported to have
  hacked into the database in an illegal act
  `r citep(bib[["OKeefe2007"]])`.  The subsequent investigation by the
  data custodians led to a wide ranging modification to the procedures
  for approval and provision of these data that make the access much
  more restricted.  Appropriate access to data is therefore required to
  address this issue. In the work reported in the conference
  presentation in this thesis, a range of available workflow tools for
  data management and analysis were investigated and developed.
#+end_src
** reports
#+begin_src R :session **R** :tangle swish_data_management_procedures/swish-dmp-report.Rmd :exports none :eval no
  
  ## Reports 
  
  Reproducible research reports are written using a scripting language
  for statistical computing and graphics. The report is made up of
  ordinary text written in a suitable format that enables the
  computational process to recognise it as text. An example is the
  Rmarkdown format which is very similar to text used when authoring
  word processor documents (http://rmarkdown.rstudio.com). There are
  also chunks of pure statistical programming code (such as R codes)
  that perform data manipulations and analyses when the document is
  'evaluated'.  When the processing stage is run a report document is
  generated that includes both content as well as the output of any
  embedded computer code 'chunks' within the document.  An example of
  this is provided in the Supporting Information document of Paper 1 of
  this thesis.
   
  
#+end_src
** DONE rational for naming and shaming Dr Begg
Hi Steve,

Good point, I actually want to attribute this data misuse to the researchers not the school.
It seems to me I can only really pin this directly to Dr Begg, per the article http://www.theaustralian.com.au/higher-education/hackers-pick-up-uq-cash-prize/story-e6frgcjx-1111113191659

"Stephen Begg was the person most responsible for the breaches...
...Associate Professor Vos, though not a user of confidential unit record files, cannot apply for access until June 1...
...It was not made clear in the letter whether Professor Vos was involved in the hacking. The university would not comment..."

I think I need to name him because this puts Begg up there with the horror stories I cite in another section (using authors names) suggesting misconduct during publication ('insert statistical method here' and 'where are the data?  Emma just make one up').  This is probably as significant a data misuse as the much touted LaCour or Potti falsifications.

I also think the magnitude of the impact on pop health researchers over the last several years is worth noting, but do you know if I overstate the importance of the hacking when it came to the coroners locking up the data?  As I recall the conversations we had with staff at the Dept Justice a couple years back, they verbally implicated that incident as the watershed moment when they decided no-one would access the mortality data for several years... but I am not sure it is actually on the formal record as such and I might overstep bounds of libel by making the direct link from Begg's hacking to Coroner's data lock-down?

I say: "Restrictions around access to data have increased recently in
Australia.  As an example the custodians of the national mortality
database made it virtually impossible to access these data for several
years after the discovery of an incident in which Australian
population health researcher Dr Stephen Begg had hacked into the
database in an illegal act (O'Keefe 2007).  The subsequent
investigation by the data custodians led to a wide ranging
modification to the procedures for approval and provision of these
data that make the access much more restricted."



On Thu, Dec 3, 2015 at 10:00 AM, Steven McEachern <steven.mceachern@anu.edu.au> wrote:

    HI Ivan,

    Iâ€™ve had a closer look, and this looks ok to me.

    The only thing Iâ€™d change is the specific naming of the UQ School of Population Health. Itâ€™s important to point out the hacking and itâ€™s implications, but Iâ€™d probably just leave the reference and citation rather than naming and shaming in the paper itself (unless you want to name the hackers as well?). 

    Cheers,
    Steve

** snip-code backups security and the role of RRR in EDA and QC

Who decides?  In many cases people preserve data 'in case' and, if
  so, some of it turn out to be considered junk that later causes a
  problem. 


### Code

In this section the basic scripted workflow is described using R
and STATA as example.  I might mention workflow software such as
Kepler, VisTrails, Taverna, Ruffus.  I might also mention electronic
notebooks and IDE such as Rstudio, Notepad++, Emacs, Eclipse and
IPython (Jupyter). But these things are outside the scope of this
paper.

- TODO
### Backups and security

Where possible, copies need to be stored in multiple places to avoid
the consequences of catastrophes such as fire or flooding
`r citep(c(bib[["Hook2010"]], bib[["White2013b"]]))`.

- TODO maybe add version control here?

## The purpose of RRReports
  The purpose of this is to transparently document the examination of,
  and any modifications to the data that have been provided into the
  analysis pipeline. The motivation for examining data and 
  modifying its structure is borne out of the need to publish data that
  is understandable to the end user and as free from inconsistencies as
  possible.

** TODO tracking the transformation from measured data to derived data
- This chunk will talk about Step 4 of Borer: 'make corrections within a scripted language'
- Data_provided and data_derived
- Leap off from here into the discussion of Wide and Long, link to the EML discussion
** table of steps
#+name:pres
#+begin_src R :session **R** :tangle no :exports none :eval no
    
    library(stringr)
    steps <- read.csv(textConnection('
    CLUSTER ,  STEP    , INPUTS                  , OUTPUTS                   
    A  ,  Step1      , "Input 1, Input 2"       , "Output 1"                 
    A  ,  Step2      , Input 3                  , Output 2                   
    B  ,  Step3      , "Output 1, Output 2"      , Output 3                  
    '), stringsAsFactors = F, strip.white = T)
    write.csv(steps, "steps_basic_workflow.csv", row.names = F)
#+end_src
** COMMENT steps_basic_workflow
#+name:steps_basic_workflow
#+begin_src R :session **R** :tangle steps_basic_workflow.R :exports none :eval no
  library(disentangle); library(stringr); library(readxl)
  steps <- read_excel("steps_basic_workflow.xlsx")
  nodes <- newnode(indat = steps, names_col = "STEP",
                   in_col = "INPUTS",out_col = "OUTPUTS")
  DiagrammeR::grViz(nodes)
  
#+end_src

** planning and implementing a pipeline
#+begin_src R :session **R** :tangle swish_data_management_procedures/swish-dmp-report.Rmd :exports none :eval no
  
  
  
  # Planning and implementing a pipeline
  
  It can be much easier to conceptualise a complicated data analysis
  method than to implement this as a reproducible research pipeline. The
  most effective way to implement a pipeline is by methodically tracking
  each of the steps taken, the data inputs needed and all the outputs of
  the step.  If done in a disciplined way then the analyst or some other
  person could 'audit' the procedure easily and access the details of
  the pipeline they need to scrutinise.
  
  ## A standardised data analysis pipeline framework
  
  One method that was selected for use in the papers of this thesis was
  the concept of the Load-Clean-Functions-Do (LCFD) framework.  This was
  first proposed by Josh Reich on the open-source software discussion
  forum called 'stack overflow' (http://stackoverflow.com/a/1434424),
  and then encoded into the 'makeProject' R package
  (http://cran.r-project.org/web/packages/makeProject/makeProject.pdf).  The approach is demonstrated in case study 2 below.
  
  \clearpage
#+end_src
** notes on planning (peng)
#+begin_src R :session **R** :tangle no :exports none :eval no


  
  ## Planning a pipeline
  
  - This section is still TODO
  
  The ordering of the process is roughly (after `r citet(bib[["Peng"]])`):
  
  1. Decide on a research question 
  1. Select a modelling framework 
  1. Conceptualise the ideal analysis data
  1. Acquire and pre process the measured data 
  1. Model selection
  1. Sensitivity analysis
  1. Data checking
  1. Reporting
  1. Distribution of code and data.
#+end_src
** case study 2 makeProject
#+begin_src R :session **R** :tangle swish_data_management_procedures/swish-dmp-report.Rmd :exports none :eval no

  ## Case study 2: Simple pipeline using the makeProject package 
  ```{r, eval = F}
  # in an interactive R session at the command line choose your project directory
  setwd("~/projects")   
  # load the required functions from the makeProject package
  library(makeProject)
  # use the makeProject function to 
  makeProject("my_first_pipelines_project")
  
  ### gives
  /my_first_pipelines_project/
      /code/**.R
      /data/
      /DESCRIPTION
      /main.R
  
  # in main.R you put these lines into the script and run them as the steps of the pipeline evolve
  source("code/load.R")
  source("code/clean.R")
  source("code/func.R")
  source("code/do.R")

  # Reporting is then a matter of choice
  ## If using the rmarkdown approach there would be an Rmd file that contained the prose
  ## and turned into a PDF, HTML or Word document with a line such as 
  rmarkdown::render("My-Pipeline-Report.Rmd", "pdf_document")
  ```
#+end_src
** COMMENT snip more complicated workflows
#+name:snip
#+begin_src R :session **R** :tangle no :exports none :eval no
  ### More complicated pipeline framework for data analysis
  - http://projecttemplate.net
  
  ```{r, eval = F}
  /project/
      /cache/
      /config/
      /data/
      /diagnostics/
      /doc/
      /graphs/
      /lib/
          /helpers.R
      /logs/
      /munge/
      /profiling/
          /01_profile.R
      /reports/
      /src/
          /01_EDA.R
          /02_clean.R
          /03_do.R
      /tests/
          /01_tests.R
      /README
      /TODO
  ```
  
  
  ### An example of a more personalised pipeline  
  - Long 2008 wrote a book recommended folder structure for statistical programmers 
  - http://www.indiana.edu/~jslsoc/web_workflow/wf_home.html
  - Recently updated with Long, S. (2015). Workflow for Reproducible Results. IV : Managing digital assets Workflow for Tools for your WF. http://txrdc.tamu.edu/documents/WFtxcrdc2014_4-digital.pdf
  
  ```{r, eval = F}
  \ProjectAcronym
      \- History starting YYYY-MM-DD
      \- Hold then delete 
      \Admin
      \Documentation 
      \Posted
           \Paper 1
               \Correspondence 
               \Text
               \Analysis
      \PrePosted 
      \Resources 
      \Write 
      \Work
  ```
#+end_src

** File organization and naming
#+begin_src R :session **R** :tangle swish_data_management_procedures/swish-dmp-report.Rmd :exports none :eval no    
  ## File organization and naming
  
  In many stages of a pipeline, an analyst will want to include details
  of the settings or what dataset they started out with. Rather than
  saving a folder or file name that is long and uninformative there are
  many different ways to organizing folders and files.
  
  Key techniques for this are available and known in the data analysis
  community as 'Tidy Data' guidelines.  In the words of 
  `r citet(bib[["WickhamRstudio2014"]])` the order that data should be
  arranged in follows some generic principles:
  
  \begin{quote}
  'A good ordering makes it easier to scan the raw values. One way of
  organizing variables is by their role in the analysis: are values
  fixed by the design of the data collection, or are they measured
  during the course of the experiment? Fixed variables describe the
  experimental design and are known in advance. Computer scientists
  often call fixed variables dimensions, and statisticians usually
  denote them with subscripts on random variables. Measured variables
  are what we actually measure in the study. Fixed variables should come
  first, followed by measured variables, each ordered so that related
  variables are contiguous. Rows can then be ordered by the first
  variable, breaking ties with the second and subsequent (fixed)
  variables.'
  \end{quote}
  
  ### An exemplar
  
  The following protocol was developed for an ecology and biodiversity database that the author of this PhD thesis was involved with.  The naming convention relied heavily on a sequence of information being used to order the names of folders, subfolders and files.  This is:
  
  1. The project name (and optional sub-project name)
  1. Data type (such as experimental unit, observational unit, and/or measurement methods)
  1. Geographic location (State, Country)
  1. Temporal frequency and coverage (such as annual or seasonal tranches).
  
  ### The concepts of slow moving dimensions and fast moving variables
  
  The concept of dimensions and variables can be useful here, and especially for deciding on filenames.  Dimensions are fixed or change slowly while variables change more quickly.  By 'change', this  means that there are more of them. For example the project name is 'fixed', that is it does not change across the files, but the sub-project name does change, just more slowly (say there may be 2-3 different sub-projects within a project). Then there may be a set of data types, and these 'change' more quickly than the sub-project name.  Then the geographic and temporal variables might change quickest of all.
  
  So a general rule for the order of things can be stated. The fixed and slowly changing variables should come first (those things that don't change, or don't change much), 
  followed by the more fluid variables (or things that change more across the project). 
  List elements can then be ordered so that the groups of things that are similar will always be contiguous, and vary sequentially within clusters.
  
  An example is shown in Table \ref{tab:TableFiles} to describe this and make it easier to understand.  Here is a set of file names that were constructed for an ecological field sites project that the I worked on ([http://www.supersites.net.au/](http://www.supersites.net.au/)).  That project involved ecological data sampled at plot-based measurement locations.  At the begining of the procedure a controlled vocabulary of data types and their acronyms was created.
  
  
#+end_src
** Code for exemplar on file naming
#+name:snip
#+begin_src R :session **R** :tangle swish_data_management_procedures/swish-dmp-report.Rmd :exports none :eval no
  
  ```{r results='asis', echo=F, eval = F}
  library(stringr)
  steps <- read.table(textConnection(
  "Filename                                                           \t Title                                                                                                                                 
  asn_fnqr_soil_charact_robson_2011.csv                               \tSoil Data, Far North Queensland Rainforest SuperSite, Robson Creek, 2011                                                             
  asn_fnqr_soil_pit_robson_2012.csv                                   \tSoil Pit Data, Water Content and Temperature, Far North Queensland Rainforest SuperSite, Robson Creek, 2012                          
  asn_fnqr_veg_seedling_robson_2010-2012.csv                          \tSeedling Survey,  Far North Queensland Rainforest SuperSite, Robson Creek, 2010-2012                                                 
  asn_fnqr_veg_seedling_transect_coord_robson_2010-2012.csv           \tSeedling Survey,  Far North Queensland Rainforest SuperSite, Robson Creek, 2010-2012                                                 
  asn_fnqr_core_1ha_robson_2014.csv                                   \tSoil Pit Data, Soil Characterisation, Far North Queensland Rainforest SuperSite, Robson Creek, Core 1 ha plot, 2014                  
  asn_fnqr_fauna_biodiversity_ctbcc_2012.csv                          \tVertebrate Fauna Biodiversity Monitoring, Far North Queensland Rainforest SuperSite, CTBCC, 2012                                     
  asn_fnqr_fauna_biodiversity_ctbcc_2013.csv                          \tVertebrate Fauna Biodiversity Monitoring, Far North Queensland Rainforest SuperSite, CTBCC, 2013                                     
  asn_fnqr_fauna_biodiversity_ctbcc_capetrib_2014.csv                 \tAvifauna Monitoring, Far North Queensland Rainforest SuperSite, Cape Tribulation, 2014                                               
  asn_fnqr_fauna_biodiversity_ctbcc_lu11a_2014.csv                    \tVertebrate Fauna Biodiversity Monitoring, Far North Queensland Rainforest SuperSite, CTBCC, LU11A, 2014                              
  asn_fnqr_fauna_biodiversity_ctbcc_lu7a_2014.csv                     \tVertebrate Fauna Biodiversity Monitoring, Far North Queensland Rainforest SuperSite, CTBCC, LU7A, 2014                               
  asn_fnqr_fauna_biodiversity_ctbcc_lu7b_2014.csv                     \tVertebrate Fauna Biodiversity Monitoring, Far North Queensland Rainforest SuperSite, CTBCC, LU7B, 2014                               
  asn_fnqr_fauna_biodiversity_ctbcc_lu9a_2014.csv                     \tVertebrate Fauna Biodiversity Monitoring, Far North Queensland Rainforest SuperSite, CTBCC, LU9A, 2014                               
  asn_fnqr_fauna_biodiversity_ctbcc-lu11a_2009-2011.csv               \tVertebrate Fauna Biodiversity Monitoring, Far North Queensland Rainforest SuperSite, CTBCC, LU11A, 2009-2011                         
  asn_fnqr_fauna_biodiversity_ctbcc-lu7a_2009-2011.csv                \tVertebrate Fauna Biodiversity Monitoring, Far North Queensland Rainforest SuperSite, CTBCC, LU7A, 2009-2011                          
  asn_fnqr_fauna_biodiversity_ctbcc-lu9a_2009-2011.csv                \tVertebrate Fauna Biodiversity Monitoring, Far North Queensland Rainforest SuperSite, CTBCC, LU9A, 2009-2011                          
  asn_fnqr_fauna_biodiversity_habitat codes_ctbcc-lu11a_2009-2011.pdf \tVertebrate Fauna Biodiversity Monitoring, Far North Queensland Rainforest SuperSite, CTBCC, LU11A, 2009-2011                         
  "), strip.white = T, sep = "\t", stringsAsFactor = F, header = T)
  #steps
  
  tabcode <- xtable(steps, caption = 'An example of standardised filename conventions to simplify tracking complicated datasets', label = 'tab:TableFiles')
  align(tabcode) <-  c( 'l', 'p{3in}','p{3in}' )
  #sink(paste(fname, '.tex',sep = ""))
  #cat(txt)
  print(tabcode,  include.rownames = F, table.placement = '!h',
   caption.placement = 'top', comment = F) #, type = "html")
  
  ```

  \begin{table}[!h]
  \tiny
  \centering
  \caption{An example of standardised filename conventions to simplify tracking complicated datasets} 
  \label{tab:TableFiles}
  \begin{tabular}{p{3.3in}p{3in}}
    \hline
  Filename & Title \\ 
    \hline
  asn\_fnqr\_soil\_charact\_robson\_2011.csv & Soil Data, Far North Queensland Rainforest SuperSite, Robson Creek, 2011 \\ 
    asn\_fnqr\_soil\_pit\_robson\_2012.csv & Soil Pit Data, Water Content and Temperature, Far North Queensland Rainforest SuperSite, Robson Creek, 2012 \\ 
    asn\_fnqr\_veg\_seedling\_robson\_2010-2012.csv & Seedling Survey,  Far North Queensland Rainforest SuperSite, Robson Creek, 2010-2012 \\ 
    asn\_fnqr\_veg\_seedling\_transect\_coord\_robson\_2010-2012.csv & Seedling Survey,  Far North Queensland Rainforest SuperSite, Robson Creek, 2010-2012 \\ 
    asn\_fnqr\_core\_1ha\_robson\_2014.csv & Soil Pit Data, Soil Characterisation, Far North Queensland Rainforest SuperSite, Robson Creek, Core 1 ha plot, 2014 \\ 
    asn\_fnqr\_fauna\_biodiversity\_ctbcc\_2012.csv & Vertebrate Fauna Biodiversity Monitoring, Far North Queensland Rainforest SuperSite, CTBCC, 2012 \\ 
    asn\_fnqr\_fauna\_biodiversity\_ctbcc\_2013.csv & Vertebrate Fauna Biodiversity Monitoring, Far North Queensland Rainforest SuperSite, CTBCC, 2013 \\ 
    asn\_fnqr\_fauna\_biodiversity\_ctbcc\_capetrib\_2014.csv & Avifauna Monitoring, Far North Queensland Rainforest SuperSite, Cape Tribulation, 2014 \\ 
    asn\_fnqr\_fauna\_biodiversity\_ctbcc\_lu11a\_2014.csv & Vertebrate Fauna Biodiversity Monitoring, Far North Queensland Rainforest SuperSite, CTBCC, LU11A, 2014 \\ 
    asn\_fnqr\_fauna\_biodiversity\_ctbcc\_lu7a\_2014.csv & Vertebrate Fauna Biodiversity Monitoring, Far North Queensland Rainforest SuperSite, CTBCC, LU7A, 2014 \\ 
    asn\_fnqr\_fauna\_biodiversity\_ctbcc\_lu7b\_2014.csv & Vertebrate Fauna Biodiversity Monitoring, Far North Queensland Rainforest SuperSite, CTBCC, LU7B, 2014 \\ 
    asn\_fnqr\_fauna\_biodiversity\_ctbcc\_lu9a\_2014.csv & Vertebrate Fauna Biodiversity Monitoring, Far North Queensland Rainforest SuperSite, CTBCC, LU9A, 2014 \\ 
    asn\_fnqr\_fauna\_biodiversity\_ctbcc-lu11a\_2009-2011.csv & Vertebrate Fauna Biodiversity Monitoring, Far North Queensland Rainforest SuperSite, CTBCC, LU11A, 2009-2011 \\ 
    asn\_fnqr\_fauna\_biodiversity\_ctbcc-lu7a\_2009-2011.csv & Vertebrate Fauna Biodiversity Monitoring, Far North Queensland Rainforest SuperSite, CTBCC, LU7A, 2009-2011 \\ 
    asn\_fnqr\_fauna\_biodiversity\_ctbcc-lu9a\_2009-2011.csv & Vertebrate Fauna Biodiversity Monitoring, Far North Queensland Rainforest SuperSite, CTBCC, LU9A, 2009-2011 \\ 
    asn\_fnqr\_fauna\_biodiversity\_habitat\_codes\_ctbcc-lu11a\_2009-2011.pdf & Vertebrate Fauna Biodiversity Monitoring, Far North Queensland Rainforest SuperSite, CTBCC, LU11A, 2009-2011 \\ 
     \hline
  \end{tabular}
  \end{table}

  
  \clearpage
#+end_src
** snip-key issues

### Key issues

- How to keep data organized and easy to reuse at a later date (including in-house reuse)
- Data storage/archiving! It's frightening how hard it can be to find raw data from a project - or if you can find it at all?
- How to organize your data with informative naming scheme and minimize duplication or lots of sub-dir
- Maybe a data citation unit such as Digital Object Identifier (DOI) that can provide links to data, along with good names and folder layout
- Concern is how to deal with in house data before you start your analysis, this is critical for reproducible research - so that you could go back and reproduce your analysis
- Recommended by some is 'convention over configuration' advice, and links to evidence based recommended filing systems. 

** Visualisation techniques

#+begin_src R :session **R** :tangle swish_data_management_procedures/swish-dmp-report.Rmd :exports none :eval no
  
  
  
  # Visualisation techniques
  
  ## Make a list of steps, inputs and outputs
  
  A very simple example of a pipeline is shown in Table
  \ref{tab:TablePipe1}.  The steps and data listed in Table
  \ref{tab:TablePipe1} can be visualised using the `newnode` function
  described below in case study 3.  This creates the graph of this pipeline
  shown in Figure \ref{fig:FigSteps}.  As the analysis progresses
  through the phases of testing, refinement and final versions. The
  linked table and graphical depiction can be very helpful for reference
  by the analyst.  The optional setting to define a status of each step
  (TODO, DONE, WONTDO) can be used to add colour, and show steps that
  remain to be done.  The addition of short summary descriptions are
  also very useful for orienting oneself to the required tasks and their
  priorities.  Such flow chart diagrams can be printed up on large
  sheets of paper and stuck on the wall beside a computer workstation
  for use in day-to-day work.
  
  ```{r TablePipe1, results='asis', echo=FALSE}
  #, tab.cap="This is the first example table\\label{tab:Table1}",cache=FALSE}
  library(stringr)
  steps <- read.csv(textConnection('
  CLUSTER ,  STEP    , INPUTS                  , OUTPUTS                                , DESCRIPTION                        , STATUS 
  A  ,  Step1      , "Source 1, Source 2"       , "Derived 1, QC check"                 , "This might be latitude and longitude of sites"     ,  DONE
  A  ,  Step2      , Source 3                  , Derived 2                           , This might be weather data               , DONE
  B  ,  Step3      , "Derived 1, Derived 2"      , Derived 3                             , Merging these data means they can be analysed   , TODO
  C  ,  Step4      , Derived 3                 , Model selection                              ,                                    , TODO
  C  ,  Step5      , Model selection           , Sensitivity analysis                         ,                                    , TODO
  '), stringsAsFactors = F, strip.white = T)
  
  #kable(
  dat <- steps[,c("STEP", "INPUTS", "OUTPUTS", "DESCRIPTION", "STATUS")]
  library(xtable)
  tabcode <- xtable(dat, caption = 'A table with the steps of a simple data analysis pipeline', label = 'tab:TablePipe1')
  align(tabcode) <-  c( 'l', 'p{.6in}','p{1.3in}','p{1.2in}', 'p{2in}','p{1in}' )
  #sink(paste(fname, '.tex',sep = ""))
  #cat(txt)
  print(tabcode,  include.rownames = F, table.placement = '!ht',
   caption.placement = 'top', comment = F) #, type = "html")
  #rws <- seq(1, (nrow(dat)), by = 2)
  #col <- rep("\\rowcolor[gray]{0.95}", length(rws))
  #print(tabcode, booktabs = TRUE, include.rownames = F, table.placement = '!ht',
  # caption.placement = 'top',
  # add.to.row = list(pos = as.list(rws), command = col),
  # comment=FALSE)
  
  ```
  
  
  
  ```{r echo=F, eval=F}
  library(disentangle); library(stringr)
  nodes <- newnode(indat = steps,   names_col = "STEP", in_col = "INPUTS",
    out_col = "OUTPUTS", desc_col = 'DESCRIPTION',
    nchar_to_snip = 70)
  #, todo_col = "STATUS",
  sink("thesis/steps-fig1.dot"); cat(nodes); sink()
  #DiagrammeR::grViz("steps-fig1.dot")
  system("dot -Tpdf thesis/steps-fig1.dot -o vignettes/steps-fig1.pdf")
  
  ```
  
  
  
  \begin{figure}[!h]
  \centering
  \includegraphics[width=\textwidth]{images/steps-fig1.pdf}
  \caption{A visualisation of a data analysis pipeline showing the use of colour}
  \label{fig:FigSteps}
  \end{figure}
  
  \clearpage
  
  
  
  As an example of the kinds of tangible steps such a workflow might
  entail a schematic diagram has been created and is shown in Figure \ref{fig:envepi_data_pipeline.png}.
  
  
  
  \begin{figure}[!h]
  \centering
  \includegraphics[width=\textwidth]{images/envepi_data_pipeline.pdf}
  \caption{A schematic flow chart showing the steps required to prepare and
  conduct an analysis of health, environmental and social data.}       
  \label{fig:envepi_data_pipeline.png}
  \end{figure}
  
  A high resolution version of this image is available online at [https://github.com/swish-climate-impact-assessment/swish_data_management_procedures/blob/phd_appendix/images/envepi_data_pipeline.pdf](https://github.com/swish-climate-impact-assessment/swish_data_management_procedures/blob/phd_appendix/images/envepi_data_pipeline.pdf)  

#+end_src
** Case study 3: Visualisation of methods steps using bespoke software
#+begin_src R :session **R** :tangle swish_data_management_procedures/swish-dmp-report.Rmd :exports none :eval no
  
  
  
  
  ## Case study 3: Visualisation of methods steps using bespoke software
  
  The method step is the key atomic unit of a scientific pipeline.  It consists of inputs, outputs and a rationale for why the step is taken.
  
  A simple way to keep track of the steps, inputs and outputs is shown in Table \ref{tab:TableBasic}.
  
  ```{r results='asis', echo=FALSE}
  library(stringr)
  steps <- read.csv(textConnection('
  CLUSTER ,  STEP    , INPUTS                  , OUTPUTS                   
  A  ,  Step1      , "Input 1, Input 2"       , "Output 1"                 
  A  ,  Step2      , Input 3                  , Output 2                   
  B  ,  Step3      , "Output 1, Output 2"      , Output 3                  
  '), stringsAsFactors = F, strip.white = T)
  
  #kable(
  steps <- steps[,c("STEP", "INPUTS", "OUTPUTS")]
  library(xtable)
  #print(xtable(steps, caption = 'Simple', label = 'tab:TableBasic'), type = "html")
  
  
  tabcode <- xtable(steps, caption = 'A simple table to track method steps, data inputs and outputs', label = 'tab:TableBasic')
  align(tabcode) <-  c( 'l', 'p{.6in}','p{2in}','p{2in}' )
  #sink(paste(fname, '.tex',sep = ""))
  #cat(txt)
  print(tabcode,  include.rownames = F, table.placement = '!h',
   caption.placement = 'top', comment = F) #, type = "html")
  #rws <- seq(1, (nrow(steps)), by = 2)
  #col <- rep("\\rowcolor[gray]{0.95}", length(rws))
  #print(tabcode, booktabs = TRUE, include.rownames = F, table.placement = '!ht',
  # caption.placement = 'top',
  # add.to.row = list(pos = as.list(rws), command = col),
  # comment=FALSE)
  
  ```  
  
  The steps and data listed in Table \ref{tab:TableBasic} can be
  visualised.  To achieve this an R function was written as part of this
  PhD project and is distributed in the author\'s own R package
  available on Github
  (\url{https://github.com/ivanhanigan/disentangle}).  This is the
  `newnode` function.  The function returns a string of text written in
  the `dot` language which can be rendered in R using the `DiagrammeR`
  package, or the standalone `graphviz` package.  This creates the graph
  view shown in Figure \ref{fig:FigBasic}.  Note that a new field was
  added for Descriptions as these are highly recommended.
  
  ```{r echo=T, eval=F}
  library(disentangle); library(stringr); library(readxl)
  steps <- read_excel("steps_basic_workflow.xlsx")
  nodes <- newnode(indat = steps, names_col = "STEP",
                   in_col = "INPUTS",out_col = "OUTPUTS")
  DiagrammeR::grViz(nodes)
  ```
  
  
  
  \begin{figure}[!ht]
  \centering
  \includegraphics[width=.5\textwidth]{images/fig-basic.pdf}
  \caption{A graphical view of the steps that comprise a simple data analysis pipeline}
  \label{fig:FigBasic}
  \end{figure}
  \clearpage
#+end_src

** refs
#+begin_src R :session **R** :tangle swish_data_management_procedures/swish-dmp-report.Rmd :exports none :eval no

  # References for appendix 1
  
  ```{r, echo=FALSE, message=FALSE, eval = T}
  write.bibtex(file="references.bib")
  ```
  
  
#+end_src
